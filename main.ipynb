{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TicTacToe.AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we develop a Q-learning agent that learns to play Tic-Tac-Toe autonomously by playing repeated games against another Q-learning agent. Q-learning is a type of reinforcement learning where an agent learns optimal actions based on rewards received from its environment. Here, the environment is the Tic-Tac-Toe game board, and the actions are the possible moves the agent can make.\n",
    "\n",
    "Each agent starts with no knowledge of the game. As they play, they explore different moves, learning from wins, losses, and draws. The agents use a Q-table to store \"Q-values,\" representing the quality of each possible action in a given board state. Over time, they adjust these values to maximize their chances of winning, minimizing unfavorable outcomes.\n",
    "\n",
    "Through this self-play process, the agents reinforce successful strategies and discard less effective ones, gradually mastering the game. This method highlights how reinforcement learning can develop strategies through interaction and feedback rather than predefined rules. By the end of the training, our Q-learning agent can play competitively, demonstrating a learned understanding of Tic-Tac-Toe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools as it\n",
    "import csv\n",
    "#tqdm is a package that allows you to display a progress bar for loops\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19683/19683 [00:04<00:00, 4489.81it/s]\n"
     ]
    }
   ],
   "source": [
    "symbols = [0,1,2]\n",
    "player = [1,2]\n",
    "permutations = list(it.product(symbols, repeat=9))\n",
    "\n",
    "# Removing impossible values where the difference between the number of 1s and 2s is greater than 1, cause this would mean a player has played twice in a row\n",
    "toRemove = []\n",
    "\n",
    "for perm in tqdm(permutations):\n",
    "    if perm.count(symbols[1]) - perm.count(symbols[2]) > 1:\n",
    "        toRemove.append(permutations.index(perm))\n",
    "    if perm.count(symbols[2]) - perm.count(symbols[1]) > 1:\n",
    "        toRemove.append(permutations.index(perm))\n",
    "    pass\n",
    "\n",
    "permutations = [perm for i, perm in enumerate(permutations) if i not in toRemove]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8953/8953 [00:00<00:00, 66621.49it/s]\n"
     ]
    }
   ],
   "source": [
    "toRemove = []\n",
    "\n",
    "for perm in tqdm(permutations):\n",
    "    if perm.count(0) == 0:\n",
    "        toRemove.append(permutations.index(perm))\n",
    "    pass\n",
    "\n",
    "permutations = [perm for i, perm in enumerate(permutations) if i not in toRemove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutations1 = permutations.copy()\n",
    "permutations2 = permutations.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Q-Values basis for Player1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8701/8701 [00:00<00:00, 14004.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Removing where player1 does not have to play\n",
    "toRemove = []\n",
    "for perm in tqdm(permutations1):\n",
    "    if perm.count(symbols[1]) > perm.count(symbols[2]):\n",
    "        toRemove.append(permutations1.index(perm))\n",
    "    pass\n",
    "\n",
    "permutations1 = [perm for i, perm in enumerate(permutations1) if i not in toRemove]\n",
    "\n",
    "q_values1 = np.zeros((len(permutations1),9))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Q-Values basis for Player2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8701/8701 [00:00<00:00, 17449.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# Removing where player2 does not have to play\n",
    "toRemove = []\n",
    "for perm in tqdm(permutations2):\n",
    "    if perm.count(symbols[1]) < perm.count(symbols[2]):\n",
    "        toRemove.append(permutations2.index(perm))\n",
    "    pass\n",
    "\n",
    "permutations2 = [perm for i, perm in enumerate(permutations2) if i not in toRemove]\n",
    "\n",
    "q_values2 = np.zeros((len(permutations2),9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the Beast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [04:11<00:00,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player1 wins:  415\n",
      "Player2 wins:  436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#parameters for the Q-learning that you can change\n",
    "learing_rate = 1\n",
    "discount_factor = 0.9\n",
    "exploration_rate = 0.5\n",
    "rounds = 1000\n",
    "life = 0\n",
    "\n",
    "#variables to keep track of the number of victories of each player\n",
    "victoryPlayer1 = 0\n",
    "victoryPlayer2 = 0\n",
    "\n",
    "# For each parties\n",
    "for round in tqdm(range(rounds)):\n",
    "    game = np.array([0,0,0,0,0,0,0,0,0])\n",
    "    play = 0\n",
    "    play1 = 0\n",
    "    play2 = 0\n",
    "    play3 = 0\n",
    "    play4 = 0\n",
    "    play5 = 0\n",
    "    play6 = 0\n",
    "    play7 = 0\n",
    "    playerSelected = np.random.choice(player)\n",
    "    history = np.zeros([9,9])\n",
    "    actionHistory = [0,0,0,0,0,0,0,0,0]\n",
    "    q_values_current_state = np.zeros(9)\n",
    "    rewards1 = 0\n",
    "    rewards2 = 0\n",
    "    victory = 0\n",
    "    line = 0\n",
    "\n",
    "    # While the game is not over\n",
    "    while play < 9:\n",
    "        \n",
    "        actionList = np.where(game == 0)[0]\n",
    "        history[play] = game\n",
    "   \n",
    "        if playerSelected == 1:\n",
    "            for i in range(len(permutations1)):\n",
    "                if np.array_equal(permutations1[i], game):\n",
    "                    line = i\n",
    "                    q_values_current_state = q_values1[line]\n",
    "                    break\n",
    "        elif playerSelected == 2:\n",
    "            for i in range(len(permutations2)):\n",
    "                if np.array_equal(permutations2[i], game):\n",
    "                    line = i\n",
    "                    q_values_current_state = q_values2[line]\n",
    "                    break\n",
    "\n",
    "        # Choose the action\n",
    "        if np.random.rand() < exploration_rate:\n",
    "            action = np.random.choice(actionList)\n",
    "        else:\n",
    "            q_values_current_state0 = np.array(q_values_current_state)[actionList]\n",
    "            max = np.argmax(q_values_current_state0)\n",
    "            max = actionList[max]\n",
    "            action = max\n",
    "        # Making the move\n",
    "        game[action] = playerSelected\n",
    "        actionHistory[play] = action\n",
    "        # Player1 winning condition\n",
    "        if((game[0] == 1 and game[1] == 1 and game[2] ==1) or (game[3] == 1 and game[4] == 1 and game[5] ==1) or (game[6] == 1 and game[7] == 1 and game[8] ==1) or (game[0] == 1 and game[3] == 1 and game[6] ==1) or (game[1] == 1 and game[4] == 1 and game[7] ==1) or (game[2] == 1 and game[5] == 1 and game[8] ==1) or (game[0] == 1 and game[4] == 1 and game[8] ==1) or (game[2] == 1 and game[4] == 1 and game[6] ==1)):\n",
    "            rewards1 = 1\n",
    "            rewards2 = -1\n",
    "            victory = 1\n",
    "        # Player2 winning condition\n",
    "        if((game[0] == 2 and game[1] == 2 and game[2] == 2) or (game[3] == 2 and game[4] == 2 and game[5] == 2) or (game[6] == 2 and game[7] == 2 and game[8] == 2) or (game[0] == 2 and game[3] == 2 and game[6] == 2) or (game[1] == 2 and game[4] == 2 and game[7] == 2) or (game[2] == 2 and game[5] == 2 and game[8] == 2) or (game[0] == 2 and game[4] == 2 and game[8] == 2) or (game[2] == 2 and game[4] == 2 and game[6] == 2)):\n",
    "            rewards1 = -1\n",
    "            rewards2 = 1\n",
    "            victory = 1\n",
    "        # End of the game\n",
    "        if victory ==1:\n",
    "            # If player1 wins\n",
    "            if playerSelected == 1:\n",
    "                victoryPlayer1 = victoryPlayer1 + 1\n",
    "                if play == 4:\n",
    "                    # Update the q_values for the last action of player1\n",
    "                    q_values1[line][action] = q_values1[line][action] + learing_rate * rewards1\n",
    "                    # Update the q_values for the last action of player2\n",
    "                    for i in range(len(permutations2)):\n",
    "                        if np.array_equal(permutations2[i], history[3]):\n",
    "                            play3 = i\n",
    "                            break\n",
    "                    q_values2[play3][actionHistory[3]] = q_values2[play3][actionHistory[3]] + learing_rate * rewards2\n",
    "                    # Update the q_values for a previous action of player1\n",
    "                    for i in range(len(permutations1)):\n",
    "                        if np.array_equal(permutations1[i], history[2]):\n",
    "                            play2 = i\n",
    "                            break\n",
    "                    q_values1[play2][actionHistory[2]] = q_values1[play2][actionHistory[2]] + learing_rate  * (discount_factor * rewards1 - life)\n",
    "                    # Update the q_values for a previous action of player2\n",
    "                    for i in range(len(permutations2)):\n",
    "                        if np.array_equal(permutations2[i], history[1]):\n",
    "                            play1 = i\n",
    "                            break\n",
    "                    q_values2[play1][actionHistory[1]] = q_values2[play1][actionHistory[1]] + learing_rate  * (discount_factor *  rewards2 - life)\n",
    "                    # Uptade the q_values for the very first action of player1\n",
    "                    q_values1[0, actionHistory[0]] = q_values1[0, actionHistory[0]] + learing_rate * ( discount_factor * discount_factor * rewards1 - life * 2)\n",
    "                elif play == 5:\n",
    "                    # Update the q_values for the last action of player1\n",
    "                    q_values1[line][action] = q_values1[line][action] + learing_rate * rewards1\n",
    "                    # Update the q_values for the last action of player2\n",
    "                    for i in range(len(permutations2)):\n",
    "                        if np.array_equal(permutations2[i], history[4]):\n",
    "                            play4 = i\n",
    "                            break\n",
    "                    q_values2[play4][actionHistory[4]] = q_values2[play4][actionHistory[4]] + learing_rate * rewards2\n",
    "                    # Update the q_values for a previous action of player1\n",
    "                    for i in range(len(permutations1)):\n",
    "                        if np.array_equal(permutations1[i], history[3]):\n",
    "                            play3 = i\n",
    "                            break\n",
    "                    q_values1[play3][actionHistory[3]] = q_values1[play3][actionHistory[3]] + learing_rate  * (discount_factor * rewards1 - life)\n",
    "                    # Update the q_values for a previous action of player2\n",
    "                    for i in range(len(permutations2)):\n",
    "                        if np.array_equal(permutations2[i], history[2]):\n",
    "                            play2 = i\n",
    "                            break\n",
    "                    q_values2[play2][actionHistory[2]] = q_values2[play2][actionHistory[2]] + learing_rate  * (discount_factor *  rewards2 - life)\n",
    "                    # Update the q_values for a previous action of player1\n",
    "                    for i in range(len(permutations1)):\n",
    "                        if np.array_equal(permutations1[i], history[1]):\n",
    "                            play1 = i\n",
    "                            break\n",
    "                    q_values1[play1][actionHistory[1]] = q_values1[play1][actionHistory[1]] + learing_rate  * (discount_factor * discount_factor * rewards1 - life * 2)\n",
    "                    # Uptade the q_values for the very first action of player2\n",
    "                    q_values2[0, actionHistory[0]] = q_values2[0, actionHistory[0]] + learing_rate * ( discount_factor * discount_factor * rewards2 - life * 2)\n",
    "                elif play == 6:\n",
    "                    # Update the q_values for the last action of player1\n",
    "                    q_values1[line][action] = q_values1[line][action] + learing_rate * rewards1\n",
    "                    # Update the q_values for the last action of player2\n",
    "                    for i in range(len(permutations2)):\n",
    "                        if np.array_equal(permutations2[i], history[5]):\n",
    "                            play5 = i\n",
    "                            break\n",
    "                    q_values2[play5][actionHistory[5]] = q_values2[play5][actionHistory[5]] + learing_rate * rewards2\n",
    "                    # Update the q_values for a previous action of player1\n",
    "                    for i in range(len(permutations1)):\n",
    "                        if np.array_equal(permutations1[i], history[4]):\n",
    "                            play4 = i\n",
    "                            break\n",
    "                    q_values1[play4][actionHistory[4]] = q_values1[play4][actionHistory[4]] + learing_rate  * (discount_factor * rewards1 - life)\n",
    "                    # Update the q_values for a previous action of player2\n",
    "                    for i in range(len(permutations2)):\n",
    "                        if np.array_equal(permutations2[i], history[3]):\n",
    "                            play3 = i\n",
    "                            break\n",
    "                    q_values2[play3][actionHistory[3]] = q_values2[play3][actionHistory[3]] + learing_rate  * (discount_factor *  rewards2 - life)\n",
    "                    # Update the q_values for a previous action of player1\n",
    "                    for i in range(len(permutations1)):\n",
    "                        if np.array_equal(permutations1[i], history[2]):\n",
    "                            play2 = i\n",
    "                            break\n",
    "                    q_values1[play2][actionHistory[2]] = q_values1[play2][actionHistory[2]] + learing_rate  * (discount_factor * discount_factor * rewards1 - life * 2)\n",
    "                    # Update the q_values for a previous action of player2\n",
    "                    for i in range(len(permutations2)):\n",
    "                        if np.array_equal(permutations2[i], history[1]):\n",
    "                            play1 = i\n",
    "                            break\n",
    "                    q_values2[play1][actionHistory[1]] = q_values2[play1][actionHistory[1]] + learing_rate  * (discount_factor * discount_factor * rewards2 - life * 2)\n",
    "                    # Uptade the q_values for the very first action of player\n",
    "                    q_values1[0, actionHistory[0]] = q_values1[0, actionHistory[0]] + learing_rate * ( discount_factor * discount_factor * discount_factor * rewards1 - life * 3)\n",
    "                elif play == 7:\n",
    "                    # Update the q_values for the last action of player1\n",
    "                    q_values1[line][action] = q_values1[line][action] + learing_rate * rewards1\n",
    "                    # Update the q_values for the last action of player2\n",
    "                    for i in range(len(permutations2)):\n",
    "                        if np.array_equal(permutations2[i], history[6]):\n",
    "                            play6 = i\n",
    "                            break\n",
    "                    q_values2[play6][actionHistory[6]] = q_values2[play6][actionHistory[6]] + learing_rate * rewards2\n",
    "                    # Update the q_values for a previous action of player1\n",
    "                    for i in range(len(permutations1)):\n",
    "                        if np.array_equal(permutations1[i], history[5]):\n",
    "                            play5 = i\n",
    "                            break\n",
    "                    q_values1[play5][actionHistory[5]] = q_values1[play5][actionHistory[5]] + learing_rate  * (discount_factor * rewards1 - life)\n",
    "                    # Update the q_values for a previous action of player2\n",
    "                    for i in range(len(permutations2)):\n",
    "                        if np.array_equal(permutations2[i], history[4]):\n",
    "                            play4 = i\n",
    "                            break\n",
    "                    q_values2[play4][actionHistory[4]] = q_values2[play4][actionHistory[4]] + learing_rate  * (discount_factor *  rewards2 - life)\n",
    "                    # Update the q_values for a previous action of player1\n",
    "                    for i in range(len(permutations1)):\n",
    "                        if np.array_equal(permutations1[i], history[3]):\n",
    "                            play3 = i\n",
    "                            break\n",
    "                    q_values1[play3][actionHistory[3]] = q_values1[play3][actionHistory[3]] + learing_rate  * (discount_factor * discount_factor * rewards1 - life * 2)\n",
    "                    # Update the q_values for a previous action of player2\n",
    "                    for i in range(len(permutations2)):\n",
    "                        if np.array_equal(permutations2[i], history[2]):\n",
    "                            play2 = i\n",
    "                            break\n",
    "                    q_values2[play2][actionHistory[2]] = q_values2[play2][actionHistory[2]] + learing_rate  * (discount_factor * discount_factor * rewards2 - life * 2)\n",
    "                    # Update the q_values for a previous action of player1\n",
    "                    for i in range(len(permutations1)):\n",
    "                        if np.array_equal(permutations1[i], history[1]):\n",
    "                            play1 = i\n",
    "                            break\n",
    "                    q_values1[play1][actionHistory[1]] = q_values1[play1][actionHistory[1]] + learing_rate  * (discount_factor * discount_factor * discount_factor * rewards1 - life * 3)\n",
    "                    # Uptade the q_values for the very first action of player2\n",
    "                    q_values2[0, actionHistory[0]] = q_values2[0, actionHistory[0]] + learing_rate * ( discount_factor * discount_factor * discount_factor * rewards2 - life * 3)\n",
    "                elif play == 8:\n",
    "                    # Update the q_values for the last action of player1\n",
    "                    q_values1[line][action] = q_values1[line][action] + learing_rate * rewards1\n",
    "                    # Update the q_values for the last action of player2\n",
    "                    for i in range(len(permutations2)):\n",
    "                        if np.array_equal(permutations2[i], history[7]):\n",
    "                            play7 = i\n",
    "                            break\n",
    "                    q_values2[play7][actionHistory[7]] = q_values2[play7][actionHistory[7]] + learing_rate * rewards2\n",
    "                    # Update the q_values for a previous action of player1\n",
    "                    for i in range(len(permutations1)):\n",
    "                        if np.array_equal(permutations1[i], history[6]):\n",
    "                            play6 = i\n",
    "                            break\n",
    "                    q_values1[play6][actionHistory[6]] = q_values1[play6][actionHistory[6]] + learing_rate  * (discount_factor * rewards1 - life)\n",
    "                    # Update the q_values for a previous action of player2\n",
    "                    for i in range(len(permutations2)):\n",
    "                        if np.array_equal(permutations2[i], history[5]):\n",
    "                            play5 = i\n",
    "                            break\n",
    "                    q_values2[play5][actionHistory[5]] = q_values2[play5][actionHistory[5]] + learing_rate  * (discount_factor *  rewards2 - life)\n",
    "                    # Update the q_values for a previous action of player1\n",
    "                    for i in range(len(permutations1)):\n",
    "                        if np.array_equal(permutations1[i], history[4]):\n",
    "                            play4 = i\n",
    "                            break\n",
    "                    q_values1[play4][actionHistory[4]] = q_values1[play4][actionHistory[4]] + learing_rate  * (discount_factor * discount_factor * rewards1 - life * 2)\n",
    "                    # Update the q_values for a previous action of player2\n",
    "                    for i in range(len(permutations2)):\n",
    "                        if np.array_equal(permutations2[i], history[3]):\n",
    "                            play3 = i\n",
    "                            break\n",
    "                    q_values2[play3][actionHistory[3]] = q_values2[play3][actionHistory[3]] + learing_rate  * (discount_factor * discount_factor * rewards2 - life * 2)\n",
    "                    # Update the q_values for a previous action of player1\n",
    "                    for i in range(len(permutations1)):\n",
    "                        if np.array_equal(permutations1[i], history[2]):\n",
    "                            play2 = i\n",
    "                            break\n",
    "                    q_values1[play2][actionHistory[2]] = q_values1[play2][actionHistory[2]] + learing_rate  * (discount_factor * discount_factor * discount_factor * rewards1 - life * 3)\n",
    "                    # Update the q_values for a previous action of player2\n",
    "                    for i in range(len(permutations2)):\n",
    "                        if np.array_equal(permutations2[i], history[1]):\n",
    "                            play1 = i\n",
    "                            break\n",
    "                    q_values2[play1][actionHistory[1]] = q_values2[play1][actionHistory[1]] + learing_rate  * (discount_factor * discount_factor * discount_factor * rewards2 - life * 3)\n",
    "                    # Uptade the q_values for the very first action of player1\n",
    "                    q_values1[0, actionHistory[0]] = q_values1[0, actionHistory[0]] + learing_rate * ( discount_factor * discount_factor * discount_factor * discount_factor * rewards1 - life * 4)\n",
    "            # If player2 wins\n",
    "            elif playerSelected == 2:\n",
    "                victoryPlayer2 = victoryPlayer2 + 1\n",
    "                if play == 4:\n",
    "                    # Update the q_values for the last action of player2\n",
    "                    q_values2[line][action] = q_values2[line][action] + learing_rate * rewards2\n",
    "                    # Update the q_values for the last action of player1\n",
    "                    for i in range(len(permutations1)):\n",
    "                        if np.array_equal(permutations1[i], history[3]):\n",
    "                            play3 = i\n",
    "                            break\n",
    "                    q_values1[play3][actionHistory[3]] = q_values1[play3][actionHistory[3]] + learing_rate * rewards1\n",
    "                    # Update the q_values for a previous action of player2\n",
    "                    for i in range(len(permutations2)):\n",
    "                        if np.array_equal(permutations2[i], history[2]):\n",
    "                            play2 = i\n",
    "                            break\n",
    "                    q_values2[play2][actionHistory[2]] = q_values2[play2][actionHistory[2]] + learing_rate  * (discount_factor * rewards2 - life)\n",
    "                    # Update the q_values for a previous action of player1\n",
    "                    for i in range(len(permutations1)):\n",
    "                        if np.array_equal(permutations1[i], history[1]):\n",
    "                            play1 = i\n",
    "                            break\n",
    "                    q_values1[play1][actionHistory[1]] = q_values1[play1][actionHistory[1]] + learing_rate  * (discount_factor *  rewards1 - life)\n",
    "                    # Uptade the q_values for the very first action of player2\n",
    "                    q_values2[0, actionHistory[0]] = q_values2[0, actionHistory[0]] + learing_rate * ( discount_factor * discount_factor * rewards2 - life * 2)\n",
    "                elif play == 5:\n",
    "                    # Update the q_values for the last action of player2\n",
    "                    q_values2[line][action] = q_values2[line][action] + learing_rate * rewards2\n",
    "                    # Update the q_values for the last action of player1\n",
    "                    for i in range(len(permutations1)):\n",
    "                        if np.array_equal(permutations1[i], history[4]):\n",
    "                            play4 = i\n",
    "                            break\n",
    "                    q_values1[play4][actionHistory[4]] = q_values1[play4][actionHistory[4]] + learing_rate * rewards1\n",
    "                    # Update the q_values for a previous action of player2\n",
    "                    for i in range(len(permutations2)):\n",
    "                        if np.array_equal(permutations2[i], history[3]):\n",
    "                            play3 = i\n",
    "                            break\n",
    "                    q_values2[play3][actionHistory[3]] = q_values2[play3][actionHistory[3]] + learing_rate  * (discount_factor * rewards2 - life)\n",
    "                    # Update the q_values for a previous action of player1\n",
    "                    for i in range(len(permutations1)):\n",
    "                        if np.array_equal(permutations1[i], history[2]):\n",
    "                            play2 = i\n",
    "                            break\n",
    "                    q_values1[play2][actionHistory[2]] = q_values1[play2][actionHistory[2]] + learing_rate  * (discount_factor *  rewards1 - life)\n",
    "                    # Update the q_values for a previous action of player2\n",
    "                    for i in range(len(permutations2)):\n",
    "                        if np.array_equal(permutations2[i], history[1]):\n",
    "                            play1 = i\n",
    "                            break\n",
    "                    q_values2[play1][actionHistory[1]] = q_values2[play1][actionHistory[1]] + learing_rate  * (discount_factor * discount_factor * rewards2 - life * 2)\n",
    "                    # Uptade the q_values for the very first action of player1\n",
    "                    q_values1[0, actionHistory[0]] = q_values1[0, actionHistory[0]] + learing_rate * ( discount_factor * discount_factor * rewards1 - life * 2)\n",
    "                elif play == 6:\n",
    "                    # Update the q_values for the last action of player2\n",
    "                    q_values2[line][action] = q_values2[line][action] + learing_rate * rewards2\n",
    "                    # Update the q_values for the last action of player1\n",
    "                    for i in range(len(permutations1)):\n",
    "                        if np.array_equal(permutations1[i], history[5]):\n",
    "                            play5 = i\n",
    "                            break\n",
    "                    q_values1[play5][actionHistory[5]] = q_values1[play5][actionHistory[5]] + learing_rate * rewards1\n",
    "                    # Update the q_values for a previous action of player2\n",
    "                    for i in range(len(permutations2)):\n",
    "                        if np.array_equal(permutations2[i], history[4]):\n",
    "                            play4 = i\n",
    "                            break\n",
    "                    q_values2[play4][actionHistory[4]] = q_values2[play4][actionHistory[4]] + learing_rate  * (discount_factor * rewards2 - life)\n",
    "                    # Update the q_values for a previous action of player1\n",
    "                    for i in range(len(permutations1)):\n",
    "                        if np.array_equal(permutations1[i], history[3]):\n",
    "                            play3 = i\n",
    "                            break\n",
    "                    q_values1[play3][actionHistory[3]] = q_values1[play3][actionHistory[3]] + learing_rate  * (discount_factor *  rewards1 - life)\n",
    "                    # Update the q_values for a previous action of player2\n",
    "                    for i in range(len(permutations2)):\n",
    "                        if np.array_equal(permutations2[i], history[2]):\n",
    "                            play2 = i\n",
    "                            break\n",
    "                    q_values2[play2][actionHistory[2]] = q_values2[play2][actionHistory[2]] + learing_rate  * (discount_factor * discount_factor * rewards2 - life * 2)\n",
    "                    # Update the q_values for a previous action of player1\n",
    "                    for i in range(len(permutations1)):\n",
    "                        if np.array_equal(permutations1[i], history[1]):\n",
    "                            play1 = i\n",
    "                            break\n",
    "                    q_values1[play1][actionHistory[1]] = q_values1[play1][actionHistory[1]] + learing_rate  * (discount_factor * discount_factor * rewards1 - life * 2)\n",
    "                    # Uptade the q_values for the very first action of player2\n",
    "                    q_values2[0, actionHistory[0]] = q_values2[0, actionHistory[0]] + learing_rate * ( discount_factor * discount_factor * rewards2 - life * 2)\n",
    "                elif play == 7:\n",
    "                    # Update the q_values for the last action of player2\n",
    "                    q_values2[line][action] = q_values2[line][action] + learing_rate * rewards2\n",
    "                    # Update the q_values for the last action of player1\n",
    "                    for i in range(len(permutations1)):\n",
    "                        if np.array_equal(permutations1[i], history[6]):\n",
    "                            play6 = i\n",
    "                            break\n",
    "                    q_values1[play6][actionHistory[6]] = q_values1[play6][actionHistory[6]] + learing_rate * rewards1\n",
    "                    # Update the q_values for a previous action of player2\n",
    "                    for i in range(len(permutations2)):\n",
    "                        if np.array_equal(permutations2[i], history[5]):\n",
    "                            play5 = i\n",
    "                            break\n",
    "                    q_values2[play5][actionHistory[5]] = q_values2[play5][actionHistory[5]] + learing_rate  * (discount_factor * rewards2 - life)\n",
    "                    # Update the q_values for a previous action of player1\n",
    "                    for i in range(len(permutations1)):\n",
    "                        if np.array_equal(permutations1[i], history[4]):\n",
    "                            play4 = i\n",
    "                            break\n",
    "                    q_values1[play4][actionHistory[4]] = q_values1[play4][actionHistory[4]] + learing_rate  * (discount_factor *  rewards1 - life)\n",
    "                    # Update the q_values for a previous action of player2\n",
    "                    for i in range(len(permutations2)):\n",
    "                        if np.array_equal(permutations2[i], history[3]):\n",
    "                            play3 = i\n",
    "                            break\n",
    "                    q_values2[play3][actionHistory[3]] = q_values2[play3][actionHistory[3]] + learing_rate  * (discount_factor * discount_factor * rewards2 - life * 2)\n",
    "                    # Update the q_values for a previous action of player1\n",
    "                    for i in range(len(permutations1)):\n",
    "                        if np.array_equal(permutations1[i], history[2]):\n",
    "                            play2 = i\n",
    "                            break\n",
    "                    q_values1[play2][actionHistory[2]] = q_values1[play2][actionHistory[2]] + learing_rate  * (discount_factor * discount_factor * rewards1 - life * 2)\n",
    "                    # Update the q_values for a previous action of player2\n",
    "                    for i in range(len(permutations2)):\n",
    "                        if np.array_equal(permutations2[i], history[1]):\n",
    "                            play1 = i\n",
    "                            break\n",
    "                    q_values2[play1][actionHistory[1]] = q_values2[play1][actionHistory[1]] + learing_rate  * (discount_factor * discount_factor * rewards2 - life * 2)\n",
    "                    # Uptade the q_values for the very first action of player1\n",
    "                    q_values1[0, actionHistory[0]] = q_values1[0, actionHistory[0]] + learing_rate * ( discount_factor * discount_factor * rewards1 - life * 2)\n",
    "                elif play == 8:\n",
    "                    # Update the q_values for the last action of player2\n",
    "                    q_values2[line][action] = q_values2[line][action] + learing_rate * rewards2\n",
    "                    # Update the q_values for the last action of player1\n",
    "                    for i in range(len(permutations1)):\n",
    "                        if np.array_equal(permutations1[i], history[7]):\n",
    "                            play7 = i\n",
    "                            break\n",
    "                    q_values1[play7][actionHistory[7]] = q_values1[play7][actionHistory[7]] + learing_rate * rewards1\n",
    "                    # Update the q_values for a previous action of player2\n",
    "                    for i in range(len(permutations2)):\n",
    "                        if np.array_equal(permutations2[i], history[6]):\n",
    "                            play6 = i\n",
    "                            break\n",
    "                    q_values2[play6][actionHistory[6]] = q_values2[play6][actionHistory[6]] + learing_rate  * (discount_factor * rewards2 - life)\n",
    "                    # Update the q_values for a previous action of player1\n",
    "                    for i in range(len(permutations1)):\n",
    "                        if np.array_equal(permutations1[i], history[5]):\n",
    "                            play5 = i\n",
    "                            break\n",
    "                    q_values1[play5][actionHistory[5]] = q_values1[play5][actionHistory[5]] + learing_rate  * (discount_factor *  rewards1 - life)\n",
    "                    # Update the q_values for a previous action of player2\n",
    "                    for i in range(len(permutations2)):\n",
    "                        if np.array_equal(permutations2[i], history[4]):\n",
    "                            play4 = i\n",
    "                            break\n",
    "                    q_values2[play4][actionHistory[4]] = q_values2[play4][actionHistory[4]] + learing_rate  * (discount_factor * discount_factor * rewards2 - life * 2)\n",
    "                    # Update the q_values for a previous action of player1\n",
    "                    for i in range(len(permutations1)):\n",
    "                        if np.array_equal(permutations1[i], history[3]):\n",
    "                            play3 = i\n",
    "                            break\n",
    "                    q_values1[play3][actionHistory[3]] = q_values1[play3][actionHistory[3]] + learing_rate  * (discount_factor * discount_factor * rewards1 - life * 2)\n",
    "                    # Update the q_values for a previous action of player2\n",
    "                    for i in range(len(permutations2)):\n",
    "                        if np.array_equal(permutations2[i], history[2]):\n",
    "                            play2 = i\n",
    "                            break\n",
    "                    q_values2[play2][actionHistory[2]] = q_values2[play2][actionHistory[2]] + learing_rate  * (discount_factor * discount_factor * rewards2 - life * 2)\n",
    "                    # Update the q_values for a previous action of player1\n",
    "                    for i in range(len(permutations1)):\n",
    "                        if np.array_equal(permutations1[i], history[1]):\n",
    "                            play1 = i\n",
    "                            break\n",
    "                    q_values1[play1][actionHistory[1]] = q_values1[play1][actionHistory[1]] + learing_rate  * (discount_factor * discount_factor * rewards1 - life * 2)\n",
    "                    # Uptade the q_values for the very first action of player2\n",
    "                    q_values2[0, actionHistory[0]] = q_values2[0, actionHistory[0]] + learing_rate * ( discount_factor * discount_factor * rewards2 - life * 2)\n",
    "            break    \n",
    "        # If the game is not over reset some parameters\n",
    "        play = play + 1\n",
    "        playerSelected = 3 - playerSelected\n",
    "\n",
    "print(\"Player1 wins: \", victoryPlayer1)\n",
    "print(\"Player2 wins: \", victoryPlayer2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('q_values1.csv', 'w', newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(q_values1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('permutations1.csv', 'w', newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(permutations1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('q_values2.csv', 'w',newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(q_values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('permutations2.csv', 'w',newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(permutations2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
